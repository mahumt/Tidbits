{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape an html, get text for a particular tag <p> that is about laptop reviews\n",
    "from bs4 import BeautifulSoup\n",
    "html_file= open('index.html', 'r')\n",
    "page= html_file.read()\n",
    "soup= BeautifulSoup(page, 'html.parser')#create instance of beautifulsoup to parse document\n",
    "reviews=soup.find_all('p') #look for p tag\n",
    "for p in reviews:\n",
    "    print p.get_text()\n",
    "\n",
    "#sentinment analysis\n",
    "from textblob import TextBlob\n",
    "positive,negative= 0.0\n",
    "for p in reviews:\n",
    "    text= p.get_text()\n",
    "    sentiment= TextBlob(text).sentiment.polarity\n",
    "    if(sentiment >= 0):\n",
    "        positive+=1\n",
    "    else:\n",
    "        negative=+1\n",
    "print \"positive.review :\" ,positive\n",
    "print \"negative.review :\" ,negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping no 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PYTHON\n",
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "# specify the url\n",
    "quote_page = 'http://www.bloomberg.com/quote/SPX:IND'\n",
    "# query the website and return the html to the variable 'page;\n",
    "page = urllib2.urlopen(quote_page)\n",
    "# parse the html using beautiful soup and store in variable 'soup'\n",
    "\n",
    "#PYTHON3\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup  \n",
    "# data=[]\n",
    " for page in url:\n",
    "        r=urllib.request.open(page)\n",
    "soup = BeautifulSoup(r, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take out the $<div>$ of name and get its value by using find() <br>\n",
    "In this case, since the HTML class name is unique on this page, we can simply query $$<div class=\"name\">$$\n",
    "$name_box = soup.find('h1', attrs={'class': 'name'})$ <br>\n",
    "![webscraping.png](webscraping.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page,'html.parser')\n",
    "# we have a variable, soup, containing the HTML of the page.  we can start coding the part that extracts the data\n",
    "# Take out the <div> of name and get its value by using find()\n",
    "# In this case, since the HTML class name is unique on this page, we can simply query <div class=\"name\">\n",
    "name_box = soup.find('h1', attrs={'class': 'companyName__99a4824b'}) #<h1 class=\"companyName__99a4824b\">S&amp;P 500 Index</h1>\n",
    "#After we have the tag, we can get the data by getting its text.\n",
    "name = name_box.text.strip() # strip() is used to remove starting and trailing\n",
    "print (name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index price\n",
    "price_box = soup.find('span', attrs={'class':'priceText__1853e8a5'}) ##<span class=\"priceText__1853e8a5\">2,803.69</span>\n",
    "price = price_box.text\n",
    "print (price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "#At the bottom of your code, add the code for writing data to a csv file.\n",
    "# open a csv file with append, so old data will not be erased\n",
    "with open('index.csv', 'a') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([name, price, datetime.now()])\n",
    "\n",
    "#PYTHON3\n",
    "data.append((name, price))\n",
    "import csv \n",
    "from datetime import datetime\n",
    "\n",
    "with open('index.csv', 'a') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for name, price in data:\n",
    "        writer.writerow([name, price, datetime.now()])\n",
    "        writer.writerow('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WebScraping Multiple indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple Indexes\n",
    "quote_page = ['http://www.bloomberg.com/quote/SPX:IND', 'http://www.bloomberg.com/quote/CCMP:IND']\n",
    "#change the data extraction code into a for loop, which will process the URLs one by one and store all the data into a variable data in tuples.\n",
    "\n",
    "data = []\n",
    "for pg in quote_page:\n",
    "    page = urllib2.urlopen(pg)  # query the website and return the html to the variable ‘page’\n",
    "    soup = BeautifulSoup(page, 'html.parser') # parse the html using beautiful soap and store in variable `soup`\n",
    "    name_box = soup.find('h1', attrs={'class': 'name'}) # Take out the <div> of name and get its value\n",
    "    name = name_box.text.strip() # strip() is used to remove starting and trailing\n",
    "    price_box = soup.find('div', attrs={'class':'price'}) # get the index price\n",
    "    price = price_box.text\n",
    "    data.append((name, price)) # save the data in tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also, modify the saving section to save data row by row.\n",
    "\n",
    "# open a csv file with append, so old data will not be erased\n",
    "with open('index.csv', 'a') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for name, price in data:\n",
    "        writer.writerow([name, price, datetime.now()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
