{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://spark.apache.org/examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORDCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: wordcount <file>\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonWordCount\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
    "    counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "                  .map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(add)\n",
    "    output = counts.collect()\n",
    "    for (word, count) in output:\n",
    "        print(\"%s: %i\" % (word, count))\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAGERANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is an example implementation of PageRank. For more conventional use,\n",
    "Please refer to PageRank implementation provided by graphx\n",
    "Example Usage:\n",
    "bin/spark-submit examples/src/main/python/pagerank.py data/mllib/pagerank_data.txt 10\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: pagerank <file> <iterations>\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    print(\"WARN: This is a naive implementation of PageRank and is given as an example!\\n\" +\n",
    "          \"Please refer to PageRank implementation provided by graphx\",\n",
    "          file=sys.stderr)\n",
    "\n",
    "    # Initialize the spark context.\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonPageRank\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Loads in input file. It should be in format of:\n",
    "    #     URL         neighbor URL\n",
    "    #     URL         neighbor URL\n",
    "    #     URL         neighbor URL\n",
    "    #     ...\n",
    "    lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
    "\n",
    "    # Loads all URLs from input file and initialize their neighbors.\n",
    "    links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
    "\n",
    "    # Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
    "    ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "\n",
    "    # Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "    for iteration in range(int(sys.argv[2])):\n",
    "        # Calculates URL contributions to the rank of other URLs.\n",
    "        contribs = links.join(ranks).flatMap(\n",
    "            lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "\n",
    "        # Re-calculates URL ranks based on neighbor contributions.\n",
    "        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "\n",
    "    # Collects all URL ranks and dump them to console.\n",
    "    for (link, rank) in ranks.collect():\n",
    "        print(\"%s has rank: %s.\" % (link, rank))\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: sort <file>\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonSort\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
    "    sortedCount = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "        .map(lambda x: (int(x), 1)) \\\n",
    "        .sortByKey()\n",
    "    # This is just a demo on how to bring all the sorted data back to a single node.\n",
    "    # In reality, we wouldn't want to collect all the data to the driver node.\n",
    "    output = sortedCount.collect()\n",
    "    for (num, unitcount) in output:\n",
    "        print(num)\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A logistic regression implementation that uses NumPy to act on batches of input data using efficient matrix operations.\n",
    "In practice, one may prefer to use the LogisticRegression algorithm inML, \n",
    "as shown in examples/src/main/python/ml/logistic_regression_with_elastic_net.py.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "D = 10  # Number of dimensions\n",
    "\n",
    "\n",
    "# Read a batch of points from the input file into a NumPy matrix object. We operate on batches to\n",
    "# make further computations faster.\n",
    "# The data file contains lines of the form <label> <x1> <x2> ... <xD>. We load each block of these\n",
    "# into a NumPy array of size numLines * (D + 1) and pull out column 0 vs the others in gradient().\n",
    "def readPointBatch(iterator):\n",
    "    strs = list(iterator)\n",
    "    matrix = np.zeros((len(strs), D + 1))\n",
    "    for i, s in enumerate(strs):\n",
    "        matrix[i] = np.fromstring(s.replace(',', ' '), dtype=np.float32, sep=' ')\n",
    "    return [matrix]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: logistic_regression <file> <iterations>\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    print(\"\"\"WARN: This is a naive implementation of Logistic Regression and is\n",
    "      given as an example!\n",
    "      Please refer to examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
    "      to see how ML's implementation is used.\"\"\", file=sys.stderr)\n",
    "\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonLR\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    points = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\\\n",
    "        .mapPartitions(readPointBatch).cache()\n",
    "    iterations = int(sys.argv[2])\n",
    "\n",
    "    # Initialize w to a random value\n",
    "    w = 2 * np.random.ranf(size=D) - 1\n",
    "    print(\"Initial w: \" + str(w))\n",
    "\n",
    "    # Compute logistic regression gradient for a matrix of data points\n",
    "    def gradient(matrix, w):\n",
    "        Y = matrix[:, 0]    # point labels (first column of input file)\n",
    "        X = matrix[:, 1:]   # point coordinates\n",
    "        # For each point (x, y), compute gradient function, then sum these up\n",
    "        return ((1.0 / (1.0 + np.exp(-Y * X.dot(w))) - 1.0) * Y * X.T).sum(1)\n",
    "\n",
    "    def add(x, y):\n",
    "        x += y\n",
    "        return x\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print(\"On iteration %i\" % (i + 1))\n",
    "        w -= points.map(lambda m: gradient(m, w)).reduce(add)\n",
    "\n",
    "    print(\"Final w: \" + str(w))\n",
    "\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
